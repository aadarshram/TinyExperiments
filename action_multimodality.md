Multimodality in robotics refers to one different from the usual sense of multiple "types" of input to a model like audio, image, etc.

In robotics, it refers to multiple "modes" of actions as robot outputs being equally valid for a given state in a robotic task. In such cases, the question of interest is, then, how does a robotic model understand it and handle it?

In this tiny experiment, I play around to understand this concept better.

1. How do different types of models handle multimodality in the output space?  
[notebook](https://colab.research.google.com/drive/1QBWmQ25SbvNr9WcrRSS7BzfxMv8R8e1Q?usp=sharing)

2. Visualizing and understanding multimodality in PushT.  
[TODO]
